# Ascentrix â€” God Mode

This repository runs local-first by default. Key notes:

- LLM backend: defaults to a local Ollama-compatible endpoint (`OLLAMA_BASE_URL`, default `http://localhost:11434`).
- OpenAI/ChatGPT support is optional and documented in `docs/openai.md`.
- To start the API locally:

```bash
.venv311/bin/python -m uvicorn api.app:app --host 127.0.0.1 --port ${GODMODE_API_PORT_HOST:-5051}
```

See `docs/openai.md` for safe steps to opt into OpenAI when desired.

## Ports & Health

- API URL: `http://localhost:${GODMODE_API_PORT_HOST:-5051}`
- HUD URL: `http://localhost:${GODMODE_HUD_PORT_HOST:-5052}`

Override ports without editing `docker-compose.yml`:

```bash
export GODMODE_API_PORT_HOST=8000
export GODMODE_HUD_PORT_HOST=3000
```

Health checks:

- API: `curl -sS http://localhost:${GODMODE_API_PORT_HOST:-5051}/health`
- HUD: `curl -sS http://localhost:${GODMODE_HUD_PORT_HOST:-5052}/health`

--
Generated by the assistant to document local-first defaults and OpenAI opt-in steps.

# Update: prepared local-first LLM and docs (tiny marker)
