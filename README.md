# Ascentrix â€” God Mode

This repository runs local-first by default. Key notes:

- LLM backend: defaults to a local Ollama-compatible endpoint (`OLLAMA_BASE_URL`, default `http://localhost:11434`).
- OpenAI/ChatGPT support is optional and documented in `docs/openai.md`.
- To start the API locally:

```bash
.venv311/bin/python -m uvicorn api.app:app --host 127.0.0.1 --port 8000
```

See `docs/openai.md` for safe steps to opt into OpenAI when desired.

--
Generated by the assistant to document local-first defaults and OpenAI opt-in steps.
